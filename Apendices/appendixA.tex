\chapter{Reparto de trabajo}
\label{Appendix:Key1}

La toma de decisiones en las distintas áreas del proyecto, así como la distribución de tareas, ha sido un esfuerzo consensuado entre los miembros del equipo. Para asegurar esta cooperación, ambos miembros han hecho una serie de reuniones informales (generalmente cada una o dos semanas) para poner en común el progreso individual realizado y acordar el curso a seguir a partir de ahí.

\section{Adrián Garrido Sierra}

En lo que respecta a la realización de esta memoria, Adrián se ha encargado de realizar las siguientes tareas:

\begin{itemize}
\item Formato y cohesión mediante el uso de LaTeX.

\item Referencias bibliográficas.

\item Revisión y colaboración en el Capítulo 1: Introducción.

\item Revisión y colaboración en el Capítulo 2: Estado de la Cuestión.

\item Capítulo 3: Tecnologías y Herramientas utilizadas.

\item Aportación principal del Capítulo 4: Explicaciones.

\item Capítulo 5: Diseño de la interfaz de explicaciones.

\item Colaboración en el apartado 6.3 Arquitectura de la aplicación.

\item Sección 7.1 Conclusiones\\

\end{itemize}

Pasando a la implementación, ha sido el responsable principal de las siguientes partes del trabajo:

\begin{itemize}
\item Montaje del framework y el servidor. Esto incluye el alojamiento en Heroku.

\item Organización de la estructura de carpetas.

\item Programación de las vistas, incluyendo HTML, CSS y JavaScript relacionado.

\item Programación del controlador.

\item Programación de funciones auxiliares para el algoritmo encargado de generar los grafos de explicaciones a partir de los datos obtenidos del modelo.

\end{itemize}


\section{Diego Sánchez Muniesa}

Los siguientes puntos de la memoria han sdo elaborados por parte de Diego Sanchez Muniesa:

\begin{itemize}
\item Formato y cohesión mediante el uso de LaTeX.

\item Redacción del Capítulo 1 del proyecto: Introducción.

\item Redacción del Capítulo 2: Estado de la Cuestión.

\item Revisión y aportaciones del Capítulo 4.

\item Aportación principal del Capítulo 4: Explicaciones.

\item Capítulo 6: Redacción del Capítulo 6: Implementación.\\

\end{itemize}

En implementación, ha sido el responsable principal de las siguientes partes del trabajo:\\

Inicialmente, comencé estudiando todo lo que hace referencia al estudio del arte y sus principales conceptos: ¿Qué es la Web Semántica?, ¿qué es el concepto de Linked Data y como se relacionan?, ¿cómo se estructuran los datos y cómo acceder a ellos? y demás información referente al estado del arte.\\

Posteriormente, estuve investigando posibles opciones a elegir para poder establecer la base de datos que íbamos a usar, junto a su correspondiente plataforma o API final. Era obviamente necesario que mantuviese un modelo RDF para poder crear relaciones o links entre toda la información que necesitambamos relacionar. Entre ellas he podido testear principalmente DBpedia y MusicBrainz hasta que finalmente optamos por lo que consideramos la mejor opción: Wikidata.\\

Una vez seleccionada la base de datos que íbamos a usar en el proyecto, comencé a crear un conjunto de queries que nos brindasen información sobre canciones aleatorias, con el propósito de obtener soltura y experiencia con el lenguaje SparQL y su aplicación en Wikidata ya que tiene un manejo diferente a otras bases de datos.\\

Más tarde, me encargué de pensar un sistema que pudiese unir diferentes explicaciones de forma que, al aplicar un proceso de obtención de datos, estos pudiensen almacenarse ordenada y jerárquicamente en nodos y aristas. Esta necesidad era un punto clave ya que de no tener una estructura adecuada, la representación gráfica sería perjudicada. Con queries individuales no podría hacerse un trabajo complejo y eficiente. El primer punto fue la modificación de las prestaciones de la librería de SparqlWrapper en un módulo el cuál nos permitía la conexión con la API de Wikidata y el manejo de queries y sus respectivas respuestas. Alrededor de ese módulo se creó el modelo de la aplicación, haciendo un estudio de todas las explicaciones que nos habíamos fijado en un proceso en el cuál, se reciben dos canciones de entrada y en consecuencia, se inicia un proceso encargado de hacer todo el estudio secuencialmente hasta crear un esquema de todo lo compartido por ambas canciones.\\

Después elaboré un preprocesado de la lista inicial de canciones de LastFM el cual contaba con una larga lista de alrededor de 19 millones de canciones, muchas de ellas, repetidas o con valores incorrectos y nulos. Quedó un final de 421 canciones, después de un tratado de sus caracteres y de hacer el testeo de que podíamos obtener información relevante sobre ellas en Wikidata.\\

Por último, me encargué de elaborar pruebas del modelo y sus consiguientes actualizaciones a medida que obteníamos distintos datos en diferentes casos de uso, ya que, no siempre el formato de los datos recogidos era el correcto o a veces surgían excepciones por la falta de ellos en la base de datos.\\


